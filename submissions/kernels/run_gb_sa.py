#!/usr/bin/env python3
from __future__ import annotations

import argparse
import shutil
import subprocess
import sys
import time
from pathlib import Path

SA_OPT_SOURCE = 'import pandas as pd\nfrom decimal import Decimal, getcontext\nfrom shapely import affinity\nfrom shapely.geometry import Polygon\nfrom shapely.strtree import STRtree\nimport time\nimport multiprocessing\nimport math\nimport random\nimport os\nfrom collections import defaultdict\nimport argparse\n\n# --- Global configuration ---\ngetcontext().prec = 50\nscale_factor = Decimal(\'1e18\')\n\n# --- Core class definition ---\nclass ChristmasTree:\n    def __init__(self, center_x=\'0\', center_y=\'0\', angle=\'0\', item_id=None):\n        self.center_x = Decimal(center_x)\n        self.center_y = Decimal(center_y)\n        self.angle = Decimal(angle)\n        self.item_id = item_id\n        self.polygon = self._create_polygon()\n\n    def _create_polygon(self):\n        trunk_w = Decimal(\'0.15\'); trunk_h = Decimal(\'0.2\')\n        base_w = Decimal(\'0.7\'); base_y = Decimal(\'0.0\')\n        mid_w = Decimal(\'0.4\'); tier_2_y = Decimal(\'0.25\')\n        top_w = Decimal(\'0.25\'); tier_1_y = Decimal(\'0.5\')\n        tip_y = Decimal(\'0.8\'); trunk_bottom_y = -trunk_h\n\n        initial_polygon = Polygon([\n            (Decimal(\'0.0\') * scale_factor, tip_y * scale_factor),\n            (top_w / Decimal(\'2\') * scale_factor, tier_1_y * scale_factor),\n            (top_w / Decimal(\'4\') * scale_factor, tier_1_y * scale_factor),\n            (mid_w / Decimal(\'2\') * scale_factor, tier_2_y * scale_factor),\n            (mid_w / Decimal(\'4\') * scale_factor, tier_2_y * scale_factor),\n            (base_w / Decimal(\'2\') * scale_factor, base_y * scale_factor),\n            (trunk_w / Decimal(\'2\') * scale_factor, base_y * scale_factor),\n            (trunk_w / Decimal(\'2\') * scale_factor, trunk_bottom_y * scale_factor),\n            (-(trunk_w / Decimal(\'2\')) * scale_factor, trunk_bottom_y * scale_factor),\n            (-(trunk_w / Decimal(\'2\')) * scale_factor, base_y * scale_factor),\n            (-(base_w / Decimal(\'2\')) * scale_factor, base_y * scale_factor),\n            (-(mid_w / Decimal(\'4\')) * scale_factor, tier_2_y * scale_factor),\n            (-(mid_w / Decimal(\'2\')) * scale_factor, tier_2_y * scale_factor),\n            (-(top_w / Decimal(\'4\')) * scale_factor, tier_1_y * scale_factor),\n            (-(top_w / Decimal(\'2\')) * scale_factor, tier_1_y * scale_factor),\n        ])\n        rotated = affinity.rotate(initial_polygon, float(self.angle), origin=(0, 0))\n        return affinity.translate(\n            rotated,\n            xoff=float(self.center_x * scale_factor),\n            yoff=float(self.center_y * scale_factor)\n        )\n\n    def clone(self) -> "ChristmasTree":\n        new_tree = ChristmasTree.__new__(ChristmasTree)\n        new_tree.center_x = self.center_x\n        new_tree.center_y = self.center_y\n        new_tree.angle = self.angle\n        new_tree.item_id = self.item_id\n        new_tree.polygon = self.polygon\n        return new_tree\n\n\ndef splitmix64(x: int) -> int:\n    """Deterministic 64-bit mixer (same spirit as C++ SplitMix64)."""\n    x &= 0xFFFFFFFFFFFFFFFF\n    x = (x + 0x9E3779B97F4A7C15) & 0xFFFFFFFFFFFFFFFF\n    z = x\n    z = (z ^ (z >> 30)) * 0xBF58476D1CE4E5B9 & 0xFFFFFFFFFFFFFFFF\n    z = (z ^ (z >> 27)) * 0x94D049BB133111EB & 0xFFFFFFFFFFFFFFFF\n    z = z ^ (z >> 31)\n    return z & 0xFFFFFFFFFFFFFFFF\n\n\ndef parse_int_range(s: str):\n    """Parse \'40-80\' (inclusive) or a single int like \'12\'. Returns (min,max) or None."""\n    if s is None:\n        return None\n    s = str(s).strip()\n    if not s:\n        return None\n    if \'-\' in s:\n        a, b = s.split(\'-\', 1)\n        a = int(a.strip())\n        b = int(b.strip())\n        if a > b:\n            a, b = b, a\n        return a, b\n    v = int(s)\n    return v, v\n\n# --- Fast helper functions ---\n\n\ndef get_tree_list_side_length_fast(polygons) -> float:\n    """Fast side-length computation (float precision)."""\n    if not polygons:\n        return 0.0\n    minx, miny, maxx, maxy = polygons[0].bounds\n    for p in polygons[1:]:\n        b = p.bounds\n        if b[0] < minx: minx = b[0]\n        if b[1] < miny: miny = b[1]\n        if b[2] > maxx: maxx = b[2]\n        if b[3] > maxy: maxy = b[3]\n    return max(maxx - minx, maxy - miny) / float(scale_factor)\n\n\ndef validate_no_overlaps(polygons):\n    """Final safety check: use STRtree to detect physical overlap (avoid expensive intersection().area)."""\n    if not polygons:\n        return True\n\n    strtree = STRtree(polygons)\n\n    for i, poly in enumerate(polygons):\n        candidates = strtree.query(poly)\n\n        for cand in candidates:\n            # Shapely 1.8: query returns geometries; Shapely 2.x: often returns indices (depends on construction)\n            if hasattr(cand, "geom_type"):\n                other = cand\n                if other is poly:\n                    continue\n            else:\n                j = int(cand)\n                if j == i:\n                    continue\n                other = polygons[j]\n\n            # touches (edge/point contact) is allowed; any non-disjoint and non-touching is treated as area overlap\n            if (not poly.disjoint(other)) and (not poly.touches(other)):\n                return False\n\n    return True\n\n\ndef parse_csv(csv_path):\n    print(f\'Loading csv: {csv_path}\')\n    result = pd.read_csv(csv_path)\n    for col in [\'x\', \'y\', \'deg\']:\n        if result[col].dtype == object:\n            result[col] = result[col].astype(str).str.strip(\'s\')\n\n    # id is usually like "<group>_<item>"; keep item_id so we can preserve IDs on save.\n    result[[\'group_id\', \'item_id\']] = result[\'id\'].str.split(\'_\', n=1, expand=True)\n\n    dict_of_tree_list = {}\n    for group_id, group_data in result.groupby(\'group_id\'):\n        # iterrows -> itertuples (faster)\n        tree_list = [\n            ChristmasTree(center_x=str(row.x), center_y=str(row.y), angle=str(row.deg), item_id=str(row.item_id))\n            for row in group_data.itertuples(index=False)\n        ]\n        dict_of_tree_list[group_id] = tree_list\n    return dict_of_tree_list\n\n\ndef save_dict_to_csv(dict_of_tree_list, output_path):\n    print(f"Saving solution to {output_path}...")\n    data = []\n    sorted_keys = sorted(dict_of_tree_list.keys(), key=lambda x: int(x))\n    for group_id in sorted_keys:\n        trees = dict_of_tree_list[group_id]\n        for i, tree in enumerate(trees):\n            item_id = tree.item_id if tree.item_id is not None else str(i)\n            data.append({\n                \'id\': f"{group_id}_{item_id}",\n                \'x\': f"s{tree.center_x}",\n                \'y\': f"s{tree.center_y}",\n                \'deg\': f"s{tree.angle}",\n            })\n    df = pd.DataFrame(data)[[\'id\', \'x\', \'y\', \'deg\']]\n    df.to_csv(output_path, index=False)\n    print("Save complete.")\n\n\n# --- Simulated Annealing worker ---\n\n\ndef run_simulated_annealing(args):\n    group_id, initial_trees, max_iterations, t_start, t_end, base_seed = args\n    n_trees = len(initial_trees)\n\n    gid_int = int(group_id)\n    task_seed = splitmix64((int(base_seed) ^ (gid_int * 0x9E3779B97F4A7C15)) & 0xFFFFFFFFFFFFFFFF)\n    rng = random.Random(task_seed)\n\n    # Decide by N size\n    is_small_n = n_trees <= 50\n\n    if is_small_n:\n        effective_max_iter = max_iterations * 3\n        effective_t_start = t_start * 2.0\n        gravity_weight = 1e-4\n    else:\n        effective_max_iter = max_iterations\n        effective_t_start = t_start\n        gravity_weight = 1e-6\n\n    # Initialize state\n    state = []\n    for t in initial_trees:\n        cx_float = float(t.center_x) * float(scale_factor)\n        cy_float = float(t.center_y) * float(scale_factor)\n        state.append({\n            \'poly\': t.polygon,\n            \'cx\': cx_float,\n            \'cy\': cy_float,\n            \'angle\': float(t.angle),\n        })\n\n    current_polys = [s[\'poly\'] for s in state]\n    current_bounds = [p.bounds for p in current_polys]\n\n    scale_f = float(scale_factor)\n    inv_scale_f = 1.0 / scale_f\n    inv_scale_f2 = 1.0 / (scale_f * scale_f)\n\n    def _envelope_from_bounds(bounds_list):\n        if not bounds_list:\n            return (0.0, 0.0, 0.0, 0.0)\n        minx, miny, maxx, maxy = bounds_list[0]\n        for b in bounds_list[1:]:\n            if b[0] < minx: minx = b[0]\n            if b[1] < miny: miny = b[1]\n            if b[2] > maxx: maxx = b[2]\n            if b[3] > maxy: maxy = b[3]\n        return (minx, miny, maxx, maxy)\n\n    def _envelope_from_bounds_replace(bounds_list, replace_i: int, replace_bounds):\n        """Compute the envelope after replacing bounds_list[replace_i] without mutating the list."""\n        if not bounds_list:\n            return (0.0, 0.0, 0.0, 0.0)\n        b0 = replace_bounds if replace_i == 0 else bounds_list[0]\n        minx, miny, maxx, maxy = b0\n        for i, b in enumerate(bounds_list[1:], start=1):\n            if i == replace_i:\n                b = replace_bounds\n            if b[0] < minx: minx = b[0]\n            if b[1] < miny: miny = b[1]\n            if b[2] > maxx: maxx = b[2]\n            if b[3] > maxy: maxy = b[3]\n        return (minx, miny, maxx, maxy)\n\n    def _side_len_from_env(env):\n        minx, miny, maxx, maxy = env\n        return max(maxx - minx, maxy - miny) * inv_scale_f\n\n    # Initialize envelope & dist_sum (maintained incrementally later)\n    env = _envelope_from_bounds(current_bounds)\n    dist_sum = 0.0\n    for s in state:\n        dist_sum += s[\'cx\'] * s[\'cx\'] + s[\'cy\'] * s[\'cy\']\n\n    def energy_from(env_local, dist_sum_local):\n        side_len = _side_len_from_env(env_local)\n        normalized_dist = (dist_sum_local * inv_scale_f2) / max(1, n_trees)\n        return side_len + gravity_weight * normalized_dist, side_len\n\n    current_energy, current_side_len = energy_from(env, dist_sum)\n\n    best_state_params = [{\'cx\': s[\'cx\'], \'cy\': s[\'cy\'], \'angle\': s[\'angle\']} for s in state]\n    best_real_score = current_side_len\n\n    T = effective_t_start\n    cooling_rate = math.pow(t_end / effective_t_start, 1.0 / effective_max_iter)\n\n    for i in range(effective_max_iter):\n        progress = i / effective_max_iter\n\n        if is_small_n:\n            move_scale = max(0.005, 3.0 * (1 - progress))\n            rotate_scale = max(0.001, 5.0 * (1 - progress))\n        else:\n            move_scale = max(0.001, 1.0 * (T / effective_t_start))\n            rotate_scale = max(0.002, 5.0 * (T / effective_t_start))\n\n        idx = rng.randint(0, n_trees - 1)\n        target = state[idx]\n\n        orig_poly = target[\'poly\']\n        orig_bounds = current_bounds[idx]\n        orig_cx, orig_cy, orig_angle = target[\'cx\'], target[\'cy\'], target[\'angle\']\n\n        dx = (rng.random() - 0.5) * scale_f * 0.1 * move_scale\n        dy = (rng.random() - 0.5) * scale_f * 0.1 * move_scale\n        d_angle = (rng.random() - 0.5) * rotate_scale\n\n        rotated_poly = affinity.rotate(orig_poly, d_angle, origin=(orig_cx, orig_cy))\n        new_poly = affinity.translate(rotated_poly, xoff=dx, yoff=dy)\n        new_bounds = new_poly.bounds\n        minx, miny, maxx, maxy = new_bounds\n\n        new_cx = orig_cx + dx\n        new_cy = orig_cy + dy\n        new_angle = orig_angle + d_angle\n\n        # --- Collision detection: fall back to full scan with bbox pruning ---\n        collision = False\n        for k in range(n_trees):\n            if k == idx:\n                continue\n            ox1, oy1, ox2, oy2 = current_bounds[k]\n            if maxx < ox1 or minx > ox2 or maxy < oy1 or miny > oy2:\n                continue\n            other = current_polys[k]\n            # touches (edge/point contact) is allowed; any non-disjoint and non-touching is treated as overlap\n            if (not new_poly.disjoint(other)) and (not new_poly.touches(other)):\n                collision = True\n                break\n\n        if collision:\n            T *= cooling_rate\n            continue\n\n        # Incremental update for dist_sum\n        old_d = orig_cx * orig_cx + orig_cy * orig_cy\n        new_d = new_cx * new_cx + new_cy * new_cy\n        cand_dist_sum = dist_sum - old_d + new_d\n\n        # Incremental update for envelope: only rescan if it "breaks" current extrema\n        env_minx, env_miny, env_maxx, env_maxy = env\n        need_recompute = (\n            (orig_bounds[0] == env_minx and new_bounds[0] > env_minx) or\n            (orig_bounds[1] == env_miny and new_bounds[1] > env_miny) or\n            (orig_bounds[2] == env_maxx and new_bounds[2] < env_maxx) or\n            (orig_bounds[3] == env_maxy and new_bounds[3] < env_maxy)\n        )\n        if need_recompute:\n            cand_env = _envelope_from_bounds_replace(current_bounds, idx, new_bounds)\n        else:\n            cand_env = (\n                min(env_minx, new_bounds[0]),\n                min(env_miny, new_bounds[1]),\n                max(env_maxx, new_bounds[2]),\n                max(env_maxy, new_bounds[3]),\n            )\n\n        new_energy, new_real_score = energy_from(cand_env, cand_dist_sum)\n        delta = new_energy - current_energy\n\n        accept = False\n        if delta < 0:\n            accept = True\n        else:\n            if T > 1e-10:\n                prob = math.exp(-delta * 1000 / T)\n                accept = rng.random() < prob\n\n        if accept:\n            current_polys[idx] = new_poly\n            current_bounds[idx] = new_bounds\n            target[\'poly\'] = new_poly\n            target[\'cx\'] = new_cx\n            target[\'cy\'] = new_cy\n            target[\'angle\'] = new_angle\n\n            current_energy = new_energy\n            env = cand_env\n            dist_sum = cand_dist_sum\n\n            if new_real_score < best_real_score:\n                best_real_score = new_real_score\n                for k in range(n_trees):\n                    best_state_params[k][\'cx\'] = state[k][\'cx\']\n                    best_state_params[k][\'cy\'] = state[k][\'cy\']\n                    best_state_params[k][\'angle\'] = state[k][\'angle\']\n\n        T *= cooling_rate\n\n    final_trees = []\n    final_polys_check = []\n    for p in best_state_params:\n        cx_dec = Decimal(p[\'cx\']) / scale_factor\n        cy_dec = Decimal(p[\'cy\']) / scale_factor\n        angle_dec = Decimal(p[\'angle\'])\n        new_t = ChristmasTree(str(cx_dec), str(cy_dec), str(angle_dec))\n        final_trees.append(new_t)\n        final_polys_check.append(new_t.polygon)\n\n    if not validate_no_overlaps(final_polys_check):\n        orig_score = get_tree_list_side_length_fast([t.polygon for t in initial_trees])\n        return group_id, initial_trees, orig_score\n\n    return group_id, final_trees, best_real_score\n\n\n# --- Main logic ---\ndef main():\n    parser = argparse.ArgumentParser(description="Santa-2025 SA optimizer (Python/Shapely).")\n    parser.add_argument("--input", default="/kaggle/working/submission_.csv", help="Input CSV path")\n    parser.add_argument("--output", default="/kaggle/working/submission.csv", help="Output CSV path")\n    parser.add_argument("--iter", type=int, default=1000000, help="Base iterations per group")\n    parser.add_argument("--tstart", type=float, default=10.0, help="Start temperature")\n    parser.add_argument("--tend", type=float, default=0.01, help="End temperature")\n    parser.add_argument("--processes", default="auto", help="Process count or \'auto\'")\n    parser.add_argument("--seed", type=int, default=42, help="Deterministic base seed")\n    parser.add_argument("--range", default=None, help="Only optimize groups in inclusive range a-b")\n    parser.add_argument("--gid_min", type=int, default=None, help="Only optimize groups >= gid_min")\n    parser.add_argument("--gid_max", type=int, default=None, help="Only optimize groups <= gid_max")\n    parser.add_argument("--time_limit_sec", type=int, default=11.5 * 3600, help="Wall time limit")\n    parser.add_argument("--save_every", type=int, default=20, help="Checkpoint frequency by finished groups")\n    args,_ = parser.parse_known_args()\n\n    INPUT_CSV = args.input\n    OUTPUT_CSV = args.output\n\n    try:\n        dict_of_tree_list = parse_csv(INPUT_CSV)\n    except FileNotFoundError:\n        print(f"Error: Could not find {INPUT_CSV}.")\n        return\n\n    all_groups_sorted = sorted(dict_of_tree_list.keys(), key=lambda x: int(x), reverse=True)\n\n    gid_min = args.gid_min\n    gid_max = args.gid_max\n    r = parse_int_range(args.range)\n    if r is not None:\n        gid_min, gid_max = r\n\n    if gid_min is None:\n        gid_min = -10**18\n    if gid_max is None:\n        gid_max = 10**18\n\n    groups_to_optimize = [gid for gid in all_groups_sorted if gid_min <= int(gid) <= gid_max]\n\n    MAX_ITER = int(args.iter)\n    T_START = float(args.tstart)\n    T_END = float(args.tend)\n\n    KAGGLE_TIME_LIMIT_SEC = int(args.time_limit_sec)\n    SAVE_EVERY_N_GROUPS = int(args.save_every)\n\n    tasks = []\n    for gid in groups_to_optimize:\n        tasks.append((gid, dict_of_tree_list[gid], MAX_ITER, T_START, T_END, args.seed))\n\n    if str(args.processes).lower() == "auto":\n        num_processes = multiprocessing.cpu_count()\n    else:\n        num_processes = max(1, int(args.processes))\n\n    # Don\'t spawn more workers than tasks.\n    num_processes = min(num_processes, max(1, len(tasks)))\n\n    print(f"Starting SA on {len(tasks)}/{len(all_groups_sorted)} groups using {num_processes} processes...")\n    if gid_min != -10**18 or gid_max != 10**18:\n        print(f"Group filter: {gid_min}-{gid_max} (inclusive)")\n    print(f"Seed(base): {args.seed}")\n    print(f"Time Limit: {KAGGLE_TIME_LIMIT_SEC / 3600:.2f} hours")\n    print("Press Ctrl+C to stop early and save progress.")\n\n    start_time = time.time()\n    improved_count = 0\n    total_tasks = len(tasks)\n    finished_tasks = 0\n\n    pool = multiprocessing.Pool(processes=num_processes)\n\n    try:\n        results_iter = pool.imap_unordered(run_simulated_annealing, tasks, chunksize=1)\n\n        for result in results_iter:\n            group_id, optimized_trees, score = result\n            finished_tasks += 1\n\n            orig_polys = [t.polygon for t in dict_of_tree_list[group_id]]\n            orig_score = get_tree_list_side_length_fast(orig_polys)\n\n            status_msg = ""\n            if score < orig_score:\n                diff = orig_score - score\n                if diff > 1e-12:\n                    status_msg = f" -> Improved! (-{diff:.6f})"\n                    dict_of_tree_list[group_id] = optimized_trees\n                    improved_count += 1\n\n            elapsed_time = time.time() - start_time\n            if elapsed_time > KAGGLE_TIME_LIMIT_SEC:\n                print(\n                    f"\\n[WARNING] Time limit approach ({elapsed_time / 3600:.2f}h). "\n                    "Stopping early to save data safely."\n                )\n                pool.terminate()\n                break\n\n            if finished_tasks % SAVE_EVERY_N_GROUPS == 0:\n                print(\n                    f"   >>> Auto-saving checkpoint at "\n                    f"{finished_tasks}/{total_tasks}..."\n                )\n                save_dict_to_csv(dict_of_tree_list, OUTPUT_CSV)\n\n            print(\n                f"[{finished_tasks}/{total_tasks}] "\n                f"G:{group_id} {orig_score:.5f}->{score:.5f} {status_msg}"\n            )\n\n        pool.close()\n        pool.join()\n        print(f"\\nOptimization finished normally in {time.time() - start_time:.2f}s")\n\n    except KeyboardInterrupt:\n        print("\\n\\n!!! Caught Ctrl+C (KeyboardInterrupt) !!!")\n        print("Terminating workers and saving current progress...")\n        pool.terminate()\n        pool.join()\n    except Exception as e:\n        print(f"\\nAn error occurred: {e}")\n        pool.terminate()\n        pool.join()\n    finally:\n        print(f"Final Save. Total Improved: {improved_count}")\n        save_dict_to_csv(dict_of_tree_list, OUTPUT_CSV)\n\n\nif __name__ == \'__main__\':\n    multiprocessing.freeze_support()\n    main()\n'
GB_OPT_SOURCE = 'import pandas as pd\nfrom decimal import Decimal, getcontext\nfrom shapely import affinity\nfrom shapely.geometry import Polygon\nimport time, random\nimport multiprocessing as mp\n\n# --- Global settings ---\ngetcontext().prec = 50\nscale_factor = Decimal(\'1e18\')\nscale_factor_float = 1e18\n\n# ====== Hyperparameters: more aggressive = larger ======\nPASSES = 6                  # Multiple synchronized passes (parallel-friendly)\nEPS_IMPROVE = 1e-12\nBOUND_EPS = 1.0\n\nDEPTH = 10                   # Can be larger, but should be paired with BEAM/MAX_STATES\nBEAM = 10\nMAX_STATES = 4000\n\nRAND_TRIES = 8\nRAND_K = 50\nRANDOM_SEED = 42\n\n# ====== Parallel settings ======\nPROCESSES = max(1, mp.cpu_count() - 2)  # Leave some CPU for the OS\nCHUNKSIZE = 8                           # map chunk size; tune per machine\n\n\n# --- Core class definition ---\nclass ChristmasTree:\n    def __init__(self, center_x=\'0\', center_y=\'0\', angle=\'0\'):\n        self.center_x = Decimal(center_x)\n        self.center_y = Decimal(center_y)\n        self.angle = Decimal(angle)\n        self.polygon = self._create_polygon()\n        self.bounds = self.polygon.bounds  # (minx, miny, maxx, maxy)\n\n    def _create_polygon(self):\n        trunk_w = Decimal(\'0.15\'); trunk_h = Decimal(\'0.2\')\n        base_w = Decimal(\'0.7\'); base_y = Decimal(\'0.0\')\n        mid_w  = Decimal(\'0.4\'); tier_2_y = Decimal(\'0.25\')\n        top_w  = Decimal(\'0.25\'); tier_1_y = Decimal(\'0.5\')\n        tip_y  = Decimal(\'0.8\'); trunk_bottom_y = -trunk_h\n\n        coords = [\n            (Decimal(\'0.0\'), tip_y),\n            (top_w / 2, tier_1_y), (top_w / 4, tier_1_y),\n            (mid_w / 2, tier_2_y), (mid_w / 4, tier_2_y),\n            (base_w / 2, base_y), (trunk_w / 2, base_y),\n            (trunk_w / 2, trunk_bottom_y), (-(trunk_w / 2), trunk_bottom_y),\n            (-(trunk_w / 2), base_y), (-(base_w / 2), base_y),\n            (-(mid_w / 4), tier_2_y), (-(mid_w / 2), tier_2_y),\n            (-(top_w / 4), tier_1_y), (-(top_w / 2), tier_1_y),\n        ]\n\n        scaled_coords = [(float(x) * scale_factor_float, float(y) * scale_factor_float) for x, y in coords]\n        initial_polygon = Polygon(scaled_coords)\n\n        rotated = affinity.rotate(initial_polygon, float(self.angle), origin=(0, 0))\n        return affinity.translate(\n            rotated,\n            xoff=float(self.center_x) * scale_factor_float,\n            yoff=float(self.center_y) * scale_factor_float\n        )\n\n    def clone(self) -> "ChristmasTree":\n        new_tree = ChristmasTree.__new__(ChristmasTree)\n        new_tree.center_x = self.center_x\n        new_tree.center_y = self.center_y\n        new_tree.angle = self.angle\n        new_tree.polygon = self.polygon\n        new_tree.bounds = self.bounds\n        return new_tree\n\n\n# --- Bounds utilities ---\ndef get_bounds_side(bounds_list):\n    if not bounds_list:\n        return 0.0\n    min_x = min(b[0] for b in bounds_list)\n    min_y = min(b[1] for b in bounds_list)\n    max_x = max(b[2] for b in bounds_list)\n    max_y = max(b[3] for b in bounds_list)\n    return max(max_x - min_x, max_y - min_y) / scale_factor_float\n\ndef compute_touching_candidates(bounds_list, eps=BOUND_EPS):\n    n = len(bounds_list)\n    if n == 0:\n        return []\n    min_x = min(b[0] for b in bounds_list)\n    min_y = min(b[1] for b in bounds_list)\n    max_x = max(b[2] for b in bounds_list)\n    max_y = max(b[3] for b in bounds_list)\n    cand = []\n    for i, b in enumerate(bounds_list):\n        if (abs(b[0] - min_x) < eps or abs(b[1] - min_y) < eps or\n            abs(b[2] - max_x) < eps or abs(b[3] - max_y) < eps):\n            cand.append(i)\n    if not cand:\n        cand = list(range(n))\n    return cand\n\n\ndef choose_removal_beam_lookahead(bounds_list, depth, beam, max_states, rand_tries, rand_k, seed):\n    """\n    depth-step lookahead + beam search + random perturbations.\n    Returns (best_first_idx, side_after_first_remove).\n    """\n    rng = random.Random(seed)\n    n0 = len(bounds_list)\n    if n0 <= 1:\n        return None, 0.0\n\n    def run_once(shuffle=True, limit_k=rand_k):\n        base_cands = compute_touching_candidates(bounds_list)\n        if shuffle:\n            rng.shuffle(base_cands)\n        if limit_k and len(base_cands) > limit_k:\n            base_cands = base_cands[:limit_k]\n\n        # First layer\n        first_layer = []\n        for idx in base_cands:\n            reduced = bounds_list[:idx] + bounds_list[idx+1:]\n            s1 = get_bounds_side(reduced)\n            # (score_now, reduced_bounds, first_idx, first_s1)\n            first_layer.append((s1, reduced, idx, s1))\n\n        if not first_layer:\n            return None, float("inf")\n\n        first_layer.sort(key=lambda x: x[0])\n        frontier = first_layer[:min(beam, len(first_layer))]\n\n        # best_key: (future_best, first_s1)\n        best_key = (frontier[0][0], frontier[0][3])\n        best_first = frontier[0][2]\n        best_s1 = frontier[0][3]\n\n        states_used = len(frontier)\n\n        # Expand to depth\n        for _d in range(2, max(2, depth + 1)):\n            new_frontier = []\n            for score_now, bds, first_idx, first_s1 in frontier:\n                if len(bds) <= 1:\n                    key = (0.0, first_s1)\n                    if key < best_key:\n                        best_key, best_first, best_s1 = key, first_idx, first_s1\n                    continue\n\n                cands = compute_touching_candidates(bds)\n                if shuffle:\n                    rng.shuffle(cands)\n                if limit_k and len(cands) > limit_k:\n                    cands = cands[:limit_k]\n\n                for j in cands:\n                    nb = bds[:j] + bds[j+1:]\n                    s = get_bounds_side(nb)\n                    new_frontier.append((s, nb, first_idx, first_s1))\n                    states_used += 1\n                    if states_used >= max_states:\n                        break\n                if states_used >= max_states:\n                    break\n\n            if not new_frontier:\n                break\n\n            new_frontier.sort(key=lambda x: x[0])\n            frontier = new_frontier[:min(beam, len(new_frontier))]\n\n            cur_best = frontier[0]\n            key = (cur_best[0], cur_best[3])\n            if key < best_key:\n                best_key, best_first, best_s1 = key, cur_best[2], cur_best[3]\n\n            if states_used >= max_states:\n                break\n\n        return best_first, best_s1\n\n    # First, do one "deterministic" run\n    best_first, best_s1 = run_once(shuffle=False, limit_k=None)\n    best_key = (best_s1, best_s1)\n\n    # Multiple randomized runs: keep the best\n    for _ in range(rand_tries):\n        first, s1 = run_once(shuffle=True, limit_k=rand_k)\n        if first is None:\n            continue\n        key = (s1, s1)\n        if key < best_key:\n            best_key = key\n            best_first = first\n            best_s1 = s1\n\n    return best_first, best_s1\n\n\n# --- Parallel worker: process one N and propose an improvement for N-1 ---\ndef worker_propose(args):\n    """\n    Input:\n      (N, bounds_list, prev_best, depth, beam, max_states, rand_tries, rand_k, base_seed)\n    Output:\n      (target_gid, source_gid, remove_idx, new_side) or None\n    """\n    (N, bounds_list, prev_best, depth, beam, max_states, rand_tries, rand_k, base_seed) = args\n\n    if bounds_list is None or len(bounds_list) <= 1:\n        return None\n\n    target_gid = f"{N-1:03d}"\n    source_gid = f"{N:03d}"\n\n    seed = (base_seed * 1000003) ^ (N * 9176) ^ (len(bounds_list) * 131)\n    best_idx, best_s1 = choose_removal_beam_lookahead(\n        bounds_list, depth, beam, max_states, rand_tries, rand_k, seed\n    )\n\n    if best_idx is None:\n        return None\n\n    if best_s1 < prev_best - EPS_IMPROVE:\n        return (target_gid, source_gid, best_idx, best_s1)\n\n    return None\n\n\n# --- IO ---\ndef parse_csv(csv_path):\n    print(f\'Loading csv: {csv_path}\')\n    result = pd.read_csv(csv_path)\n\n    for col in [\'x\', \'y\', \'deg\']:\n        if result[col].dtype == object:\n            result[col] = result[col].astype(str).str.strip().str.lstrip(\'s\')\n\n    result[[\'group_id\', \'item_id\']] = result[\'id\'].astype(str).str.split(\'_\', n=2, expand=True)\n\n    dict_of_tree_list = {}\n    dict_of_side_length = {}\n\n    for group_id, group_data in result.groupby(\'group_id\'):\n        trees = []\n        for _, row in group_data.iterrows():\n            trees.append(ChristmasTree(center_x=row[\'x\'], center_y=row[\'y\'], angle=row[\'deg\']))\n        gid = f"{int(group_id):03d}"\n        dict_of_tree_list[gid] = trees\n        dict_of_side_length[gid] = get_bounds_side([t.bounds for t in trees])\n\n    return dict_of_tree_list, dict_of_side_length\n\n\ndef save_dict_to_csv(dict_of_tree_list, output_path):\n    print(f"Saving to {output_path}...")\n    data = []\n    sorted_keys = sorted(dict_of_tree_list.keys(), key=lambda x: int(x))\n    for group_id in sorted_keys:\n        trees = dict_of_tree_list[group_id]\n        for i, tree in enumerate(trees):\n            data.append({\n                \'id\': f"{group_id}_{i}",\n                \'x\': f"s{tree.center_x}",\n                \'y\': f"s{tree.center_y}",\n                \'deg\': f"s{tree.angle}"\n            })\n    pd.DataFrame(data)[[\'id\', \'x\', \'y\', \'deg\']].to_csv(output_path, index=False)\n    print("Save complete.")\n\n\ndef main():\n    INPUT_CSV = \'/kaggle/working/submission.csv\'\n    OUTPUT_CSV = \'/kaggle/working/submission.csv\'\n\n    try:\n        dict_of_tree_list, dict_of_side_length = parse_csv(INPUT_CSV)\n    except FileNotFoundError:\n        print(f"Error: Not found {INPUT_CSV}")\n        return\n\n    start_time = time.time()\n    print(\n        f"MP HARD optimization: PROCESSES={PROCESSES}, PASSES={PASSES}, "\n        f"DEPTH={DEPTH}, BEAM={BEAM}, MAX_STATES={MAX_STATES}, RAND_TRIES={RAND_TRIES}, RAND_K={RAND_K}"\n    )\n\n    # Windows compatibility: use spawn\n    ctx = mp.get_context("spawn")\n\n    changed_total = 0\n\n    with ctx.Pool(processes=PROCESSES) as pool:\n        for pass_id in range(1, PASSES + 1):\n            print(f"\\n=== PASS {pass_id}/{PASSES} ===")\n            # Snapshot: all proposals in this pass are based on the same frozen data (parallel-safe)\n            snap_tree_list = {k: v for k, v in dict_of_tree_list.items()}\n            snap_side = {k: v for k, v in dict_of_side_length.items()}\n\n            tasks = []\n            base_seed = RANDOM_SEED + pass_id * 10007\n\n            # Propose from N=200..3 to improve N-1\n            for N in range(200, 2, -1):\n                gidN = f"{N:03d}"\n                gidPrev = f"{N-1:03d}"\n                if gidN not in snap_tree_list or gidPrev not in snap_side:\n                    continue\n                bounds_list = [t.bounds for t in snap_tree_list[gidN]]\n                prev_best = snap_side[gidPrev]\n                tasks.append((N, bounds_list, prev_best, DEPTH, BEAM, MAX_STATES, RAND_TRIES, RAND_K, base_seed))\n\n            # Compute proposals in parallel\n            proposals = pool.map(worker_propose, tasks, chunksize=CHUNKSIZE)\n\n            # For each target_gid, keep the best proposal (multiple N may target the same N-1)\n            best_for_target = {}  # target_gid -> (new_side, source_gid, remove_idx)\n            for p in proposals:\n                if p is None:\n                    continue\n                target_gid, source_gid, remove_idx, new_side = p\n                cur = best_for_target.get(target_gid)\n                if cur is None or new_side < cur[0]:\n                    best_for_target[target_gid] = (new_side, source_gid, remove_idx)\n\n            # Apply improvements (single unified write-back)\n            changed_this_pass = 0\n            for target_gid, (new_side, source_gid, remove_idx) in best_for_target.items():\n                old_side = dict_of_side_length.get(target_gid, float("inf"))\n                if new_side < old_side - EPS_IMPROVE:\n                    # Remove remove_idx from the snapshot solution of source_gid to form the new solution for target_gid\n                    src_list = snap_tree_list[source_gid]\n                    new_list = src_list[:remove_idx] + src_list[remove_idx+1:]\n                    dict_of_tree_list[target_gid] = new_list\n                    dict_of_side_length[target_gid] = new_side\n                    print(f"[Group {target_gid}] Improved! {old_side:.6f} -> {new_side:.6f} (from {source_gid}, rm={remove_idx})")\n                    changed_this_pass += 1\n                    changed_total += 1\n\n            print(f"PASS {pass_id} changes: {changed_this_pass}")\n            if changed_this_pass == 0:\n                print("No changes -> early stop.")\n                break\n\n    print(f"\\nTotal changes: {changed_total}")\n    print(f"Total time: {time.time() - start_time:.2f}s")\n    save_dict_to_csv(dict_of_tree_list, OUTPUT_CSV)\n\n\nif __name__ == \'__main__\':\n    main()'


def _write_script(path: Path, content: str) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(content, encoding="utf-8")


def _run(cmd: list[str], label: str) -> None:
    print(f"==> {label}", flush=True)
    print(" ".join(cmd), flush=True)
    start = time.perf_counter()
    subprocess.run(cmd, check=True)
    elapsed = time.perf_counter() - start
    print(f"<== {label} done in {elapsed:.1f}s", flush=True)


def _select_baseline(baseline_arg: str) -> Path:
    if baseline_arg and baseline_arg != "auto":
        candidate = Path(baseline_arg)
        if candidate.exists():
            return candidate
        raise SystemExit(f"baseline not found: {candidate}")

    candidates = [
        Path("/kaggle/input/santa-2025-bbox3-baseline/best_submission.csv"),
        Path("/kaggle/input/santa-2025-bbox3-baseline/bbox3_best_submission.csv"),
        Path("/kaggle/input/santa-2025-csv/santa-2025.csv"),
    ]
    for candidate in candidates:
        if candidate.exists():
            return candidate
    tried = ", ".join(str(c) for c in candidates)
    raise SystemExit(f"baseline not found; tried: {tried}")


def main() -> None:
    parser = argparse.ArgumentParser()
    parser.add_argument("--baseline", default="auto", help="auto or explicit path")
    parser.add_argument("--work-dir", default="/kaggle/working")
    parser.add_argument("--out-dir", default="/kaggle/working/gb_sa")
    parser.add_argument("--sa-hours", type=float, default=12.0)
    parser.add_argument("--sa-iter", type=int, default=1000000)
    parser.add_argument("--sa-range", default="1-120")
    parser.add_argument("--sa-processes", default="auto")
    parser.add_argument("--sa-tstart", type=float, default=10.0)
    parser.add_argument("--sa-tend", type=float, default=0.01)
    parser.add_argument("--sa-seed", type=int, default=42)
    parser.add_argument("--sa-save-every", type=int, default=20)
    parser.add_argument("--skip-gb", action="store_true", default=False)
    args = parser.parse_args()

    work_dir = Path(args.work_dir)
    out_dir = Path(args.out_dir)
    out_dir.mkdir(parents=True, exist_ok=True)

    baseline = _select_baseline(args.baseline)
    print(f"Using baseline: {baseline}", flush=True)

    input_csv = work_dir / "submission_.csv"
    output_csv = work_dir / "submission.csv"
    shutil.copy(baseline, input_csv)
    shutil.copy(baseline, out_dir / "baseline.csv")

    sa_script = work_dir / "sa_opt.py"
    gb_script = work_dir / "gb_opt.py"
    _write_script(sa_script, SA_OPT_SOURCE)
    _write_script(gb_script, GB_OPT_SOURCE)

    sa_time_limit = int(args.sa_hours * 3600)
    sa_cmd = [
        sys.executable,
        str(sa_script),
        "--input",
        str(input_csv),
        "--output",
        str(output_csv),
        "--iter",
        str(args.sa_iter),
        "--tstart",
        str(args.sa_tstart),
        "--tend",
        str(args.sa_tend),
        "--seed",
        str(args.sa_seed),
        "--processes",
        str(args.sa_processes),
        "--range",
        str(args.sa_range),
        "--time_limit_sec",
        str(sa_time_limit),
        "--save_every",
        str(args.sa_save_every),
    ]
    _run(sa_cmd, "SA")

    if not args.skip_gb:
        gb_cmd = [sys.executable, str(gb_script)]
        _run(gb_cmd, "GB")

    final_path = out_dir / "best_submission.csv"
    shutil.copy(output_csv, final_path)

    print(f"final submission: {final_path}", flush=True)


if __name__ == "__main__":
    main()
